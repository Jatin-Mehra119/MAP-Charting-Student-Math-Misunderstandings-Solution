{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbf5978",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-16T17:26:56.600172Z",
     "iopub.status.busy": "2025-10-16T17:26:56.599922Z",
     "iopub.status.idle": "2025-10-16T17:27:10.079276Z",
     "shell.execute_reply": "2025-10-16T17:27:10.078510Z"
    },
    "papermill": {
     "duration": 13.486065,
     "end_time": "2025-10-16T17:27:10.081317",
     "exception": false,
     "start_time": "2025-10-16T17:26:56.595252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-index --find-links=/kaggle/input/transformers-4-56-1-and-deps transformers -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fd628",
   "metadata": {
    "papermill": {
     "duration": 0.002966,
     "end_time": "2025-10-16T17:27:10.087869",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.084903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hunyaun 7B 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ff8ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:27:10.095477Z",
     "iopub.status.busy": "2025-10-16T17:27:10.095254Z",
     "iopub.status.idle": "2025-10-16T17:27:10.102796Z",
     "shell.execute_reply": "2025-10-16T17:27:10.102160Z"
    },
    "papermill": {
     "duration": 0.012843,
     "end_time": "2025-10-16T17:27:10.103783",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.090940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Hunyaun_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Hunyaun_inference.py\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.le = None\n",
    "        self.isPreprocess = False\n",
    "        self.correct_lookup = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.train_df = pd.read_csv(self.args.train_path)\n",
    "        self.test_df = pd.read_csv(self.args.test_path)\n",
    "        if self.args.use_extra_data:\n",
    "            self.extra_df = pd.read_csv(self.args.extra_data_path)\n",
    "            self.train_df = pd.concat([self.train_df, self.extra_df], ignore_index=True)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        if self.isPreprocess == False:\n",
    "            return \"please preprocess first\"\n",
    "        num_class = self.train_df['label'].nunique()\n",
    "        return num_class\n",
    "\n",
    "    def get_label_encoder(self):\n",
    "        if self.le is None:\n",
    "            raise ValueError(\"LabelEncoder not initialized. Please run preprocess first.\")\n",
    "        return self.le\n",
    "\n",
    "    @staticmethod\n",
    "    def format_input(row):\n",
    "        correct_text = \"Yes\" if row['IsCorrect'] else \"No\"\n",
    "        return (\n",
    "            f\"Question: {row['QuestionText']}\\n\"\n",
    "            f\"Answer: {row['MC_Answer']}\\n\"\n",
    "            f\"Correct? {correct_text}\\n\"\n",
    "            f\"Student Explanation: {row['StudentExplanation']}\\n\"\n",
    "        )\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.load_data()\n",
    "        self.train_df['Misconception'] = self.train_df['Misconception'].fillna('NA')\n",
    "        self.train_df['target'] = self.train_df['Category'] + ':' + self.train_df['Misconception']\n",
    "\n",
    "        correct_samples = self.train_df[self.train_df['Category'].str.startswith('True', na=False)].copy()\n",
    "        correct_samples['count'] = correct_samples.groupby(['QuestionId', 'MC_Answer'])['MC_Answer'].transform('count')\n",
    "        most_popular_correct = correct_samples.sort_values('count', ascending=False).drop_duplicates(['QuestionId'])\n",
    "        self.correct_lookup = most_popular_correct[['QuestionId', 'MC_Answer']].copy()\n",
    "        self.correct_lookup['IsCorrect_flag'] = True\n",
    "\n",
    "        self.train_df = self.train_df.merge(self.correct_lookup, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "        self.train_df['IsCorrect'] = self.train_df['IsCorrect_flag'].notna()\n",
    "        self.train_df = self.train_df.drop(columns=['IsCorrect_flag'])\n",
    "\n",
    "        self.le = LabelEncoder()\n",
    "        self.train_df['label'] = self.le.fit_transform(self.train_df['target'])\n",
    "        self.train_df['text'] = self.train_df.apply(self.format_input, axis=1)\n",
    "\n",
    "        self.isPreprocess = True\n",
    "        return self.train_df\n",
    "\n",
    "    def inference_processor(self):\n",
    "        if self.isPreprocess == False:\n",
    "            return \"Have you do the train? please preprocess first\"\n",
    "        self.test_df = self.test_df.merge(self.correct_lookup, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "        self.test_df['IsCorrect'] = self.test_df['IsCorrect_flag'].notna()\n",
    "        self.test_df = self.test_df.drop(columns=['IsCorrect_flag'])\n",
    "        self.test_df['text'] = self.test_df.apply(self.format_input, axis=1)\n",
    "        return self.test_df\n",
    "\n",
    "# inference\n",
    "args = Namespace(\n",
    "    train_path='/kaggle/input/map-charting-student-math-misunderstandings/train.csv',\n",
    "    test_path='/kaggle/input/map-charting-student-math-misunderstandings/test.csv',\n",
    "    use_extra_data=False,\n",
    "    extra_data_path='no_datas.csv',\n",
    "    model_dir=\"/kaggle/input/hunyuan-7b-instruct-map\",      \n",
    "    inference_model_dir=\"/kaggle/input/hunyuan-7b-instruct-map\",\n",
    "    mode='inference',\n",
    "    model_name=\"/kaggle/input/hunyuan-7b-instruct-bf16\"   \n",
    ")\n",
    "\n",
    "DP = DataProcessor(args)\n",
    "_ = DP.preprocess()\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.inference_model_dir, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model_name,\n",
    "    num_labels=DP.get_num_classes(),\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, args.inference_model_dir)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "MAX_LEN = 256\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "test_df = DP.inference_processor()\n",
    "ds_test = Dataset.from_pandas(test_df[['text']])\n",
    "ds_test = ds_test.map(tokenize_function, batched=True)\n",
    "\n",
    "inference_args = TrainingArguments(\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    output_dir=\"./temp\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=inference_args,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Run prediction with Trainer\n",
    "pred_output = trainer.predict(ds_test)\n",
    "# pred_output.predictions -> shape (num_samples, num_classes)\n",
    "logits = pred_output.predictions\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = softmax(logits, axis=1)   # numpy array, same shape\n",
    "\n",
    "# Sort classes by probability (descending)\n",
    "top_indices = np.argsort(-probs, axis=1)\n",
    "\n",
    "# Decode labels\n",
    "le = DP.get_label_encoder()\n",
    "flat_indices = top_indices.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_indices)\n",
    "top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "\n",
    "# ---- Save Top-3 submission ----\n",
    "joined_preds = [\" \".join(row[:35]) for row in top_labels]\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test_df.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission_trainer.csv\", index=False)\n",
    "\n",
    "# ---- Save full probability table (for ensembling) ----\n",
    "prob_data = []\n",
    "num_classes = logits.shape[1]\n",
    "for i in range(len(logits)):\n",
    "    # store probability for each class in descending order\n",
    "    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(num_classes)}\n",
    "    prob_dict[\"row_id\"] = test_df.row_id.values[i]\n",
    "    prob_dict[\"top_classes\"] = \" \".join(top_labels[i, :num_classes])\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_Hunyaun_prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e742bc6",
   "metadata": {
    "papermill": {
     "duration": 0.00301,
     "end_time": "2025-10-16T17:27:10.110109",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.107099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Qwen 3 4B Inf 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ed0724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:27:10.117091Z",
     "iopub.status.busy": "2025-10-16T17:27:10.116874Z",
     "iopub.status.idle": "2025-10-16T17:27:10.122518Z",
     "shell.execute_reply": "2025-10-16T17:27:10.121903Z"
    },
    "papermill": {
     "duration": 0.010488,
     "end_time": "2025-10-16T17:27:10.123545",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.113057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_4b_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_4b_inference.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "model_name = \"/kaggle/input/qwen3-4b-map-lora-training\"\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"This answer is correct.\" if row['is_correct'] else \"This answer is incorrect.\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"{x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "# Encode targets\n",
    "le = LabelEncoder()\n",
    "train.Misconception = train.Misconception.fillna('NA')\n",
    "train['target'] = train.Category + ':' + train.Misconception\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "n_classes = len(le.classes_)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "\n",
    "# Identify correct answers to mark test rows\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0], axis=1) == 'True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId', 'MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c', ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId', 'MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "test = test.merge(correct, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "# Load model/tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"balanced\",\n",
    "    dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "ds_test = Dataset.from_pandas(test[['text']])\n",
    "ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Inference\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"deepseek\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        all_logits.append(outputs.logits.float().cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(all_logits, axis=0)\n",
    "probs = softmax(predictions, axis=1)\n",
    "top_indices = np.argsort(-probs, axis=1)\n",
    "flat_indices = top_indices.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_indices)\n",
    "top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "\n",
    "# Save top-3 submission\n",
    "joined_preds = [\" \".join(row[:3]) for row in top_labels]\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission_qwen3_4B_top3.csv\", index=False)\n",
    "\n",
    "# Save top-25 probabilities for ensembling\n",
    "prob_data = []\n",
    "for i in range(len(predictions)):\n",
    "    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}\n",
    "    prob_dict['row_id'] = test.row_id.values[i]\n",
    "    prob_dict['top_classes'] = \" \".join(top_labels[i, :25])\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_qwen3_4B_probabilities.csv\", index=False)\n",
    "\n",
    "print(\"✅ Completed - saved submission and probabilities\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f1f1a",
   "metadata": {
    "papermill": {
     "duration": 0.002885,
     "end_time": "2025-10-16T17:27:10.129529",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.126644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## EEDI Qwen 2 LoRA 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81f41fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:27:10.136529Z",
     "iopub.status.busy": "2025-10-16T17:27:10.136326Z",
     "iopub.status.idle": "2025-10-16T17:27:10.142709Z",
     "shell.execute_reply": "2025-10-16T17:27:10.142174Z"
    },
    "papermill": {
     "duration": 0.011147,
     "end_time": "2025-10-16T17:27:10.143731",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.132584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen2_8b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen2_8b.py\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "VER=1\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "DIR = f\"ver_{VER}\"\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "train.Misconception = train.Misconception.fillna('NA')\n",
    "train['target'] = train.Category+\":\"+train.Misconception\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "target_classes = le.classes_\n",
    "n_classes = len(target_classes)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "train.head()\n",
    "\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "train.is_correct = train.is_correct.fillna(0)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/edi-trained-map-qwen/best\")\n",
    "MAX_LEN = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def format_input(row):\n",
    "    x = \"Yes\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Is Correct Answer: {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "train['text'] = train.apply(format_input,axis=1)\n",
    "print(\"Example prompt for our LLM:\")\n",
    "print()\n",
    "print( train.text.values[0] )\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "COLS = ['text','label']\n",
    "train_ds = Dataset.from_pandas(train_df[COLS])\n",
    "val_ds = Dataset.from_pandas(val_df[COLS])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "columns = ['input_ids', 'attention_mask', 'label']\n",
    "train_ds.set_format(type='torch', columns=columns)\n",
    "val_ds.set_format(type='torch', columns=columns)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "Model_Name = \"/kaggle/input/qwen-2-5-map\" # Main Model\n",
    "model_name = \"/kaggle/input/edi-trained-map-qwen/best\" # LoRa adaptors\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for the LoRA checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    Model_Name, \n",
    "    num_labels=n_classes,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Resize base model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, model_name)\n",
    "\n",
    "# Cast LoRA parameters to float16\n",
    "model = model.to(dtype=torch.float16)\n",
    "\n",
    "print(next(model.parameters()).dtype)\n",
    "\n",
    "# Add a new padding token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Set the pad token id in the model's config\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "from transformers import  TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"./{DIR}\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\", #no for no saving \n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"map@3\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    bf16=False, # TRAIN WITH BF16 IF LOCAL GPU IS NEWER GPU          \n",
    "    fp16=True, # INFER WITH FP16 BECAUSE KAGGLE IS T4 GPU\n",
    ")\n",
    "# CUSTOM MAP@3 METRIC\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    \n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]  # Top 3 predictions\n",
    "    match = (top3 == labels[:, None])\n",
    "\n",
    "    # Compute MAP@3 manually\n",
    "    map3 = 0\n",
    "    for i in range(len(labels)):\n",
    "        if match[i, 0]:\n",
    "            map3 += 1.0\n",
    "        elif match[i, 1]:\n",
    "            map3 += 1.0 / 2\n",
    "        elif match[i, 2]:\n",
    "            map3 += 1.0 / 3\n",
    "    return {\"map@3\": map3 / len(labels)}\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_map3,\n",
    ")\n",
    "\n",
    "# Load test set\n",
    "test = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "# HuggingFace Dataset\n",
    "ds_test = Dataset.from_pandas(test[['text']])\n",
    "ds_test = ds_test.map(tokenize, batched=True)\n",
    "\n",
    "# Run inference\n",
    "pred_output = trainer.predict(ds_test)\n",
    "logits = pred_output.predictions\n",
    "probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# ---- Top-3 predictions ----\n",
    "top3 = np.argsort(-probs, axis=1)[:, :3]   # shape: [num_samples, 3]\n",
    "\n",
    "# Decode numeric class indices to original string labels\n",
    "flat_top3 = top3.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_top3)\n",
    "top_labels = decoded_labels.reshape(top3.shape)\n",
    "\n",
    "# Join 3 labels per row with space\n",
    "joined_preds = [\" \".join(row) for row in top_labels]\n",
    "\n",
    "# Save Top-3 submission\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission_qwen3_14b.csv\", index=False)\n",
    "\n",
    "# ---- Save Top-25 probability table (for ensembling) ----\n",
    "prob_data = []\n",
    "top_k = 25\n",
    "top_indices = np.argsort(-probs, axis=1)\n",
    "\n",
    "for i in range(len(logits)):\n",
    "    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(top_k)}\n",
    "    prob_dict[\"row_id\"] = test.row_id.values[i]\n",
    "    prob_dict[\"top_classes\"] = \" \".join(\n",
    "        le.inverse_transform(top_indices[i, :top_k])\n",
    "    )\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_qwen2_8b_prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24939ce",
   "metadata": {
    "papermill": {
     "duration": 0.003025,
     "end_time": "2025-10-16T17:27:10.149845",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.146820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Qwen 3 14B LoRA 0.944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec87ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:27:10.157146Z",
     "iopub.status.busy": "2025-10-16T17:27:10.156945Z",
     "iopub.status.idle": "2025-10-16T17:27:10.163111Z",
     "shell.execute_reply": "2025-10-16T17:27:10.162556Z"
    },
    "papermill": {
     "duration": 0.011189,
     "end_time": "2025-10-16T17:27:10.164095",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.152906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_14B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_14B.py\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "VER=1\n",
    "model_name = \"/kaggle/input/qwen3-14b-lora-map/results (1)/best\"\n",
    "EPOCHS = 2\n",
    "\n",
    "DIR = f\"ver_{VER}\"\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "train.Misconception = train.Misconception.fillna('NA')\n",
    "train['target'] = train.Category+\":\"+train.Misconception\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "target_classes = le.classes_\n",
    "n_classes = len(target_classes)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "train.head()\n",
    "\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "train = train.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "train.is_correct = train.is_correct.fillna(0)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "MAX_LEN = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def format_input(row):\n",
    "    x = \"Yes\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Is Correct Answer: {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "train['text'] = train.apply(format_input,axis=1)\n",
    "print(\"Example prompt for our LLM:\")\n",
    "print()\n",
    "print( train.text.values[0] )\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "COLS = ['text','label']\n",
    "train_ds = Dataset.from_pandas(train_df[COLS])\n",
    "val_ds = Dataset.from_pandas(val_df[COLS])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "columns = ['input_ids', 'attention_mask', 'label']\n",
    "train_ds.set_format(type='torch', columns=columns)\n",
    "val_ds.set_format(type='torch', columns=columns)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "Model_Name = \"/kaggle/input/qwen-3/transformers/14b/1\" # Main Model\n",
    "model_name = \"/kaggle/input/qwen3-14b-lora-map/results (1)/best\" # LoRa adaptors\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for the LoRA checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    Model_Name, \n",
    "    num_labels=n_classes,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Resize base model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, model_name)\n",
    "\n",
    "# Cast LoRA parameters to float16\n",
    "model = model.to(dtype=torch.float16)\n",
    "\n",
    "print(next(model.parameters()).dtype)\n",
    "\n",
    "# Add a new padding token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Set the pad token id in the model's config\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "from transformers import  TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"./{DIR}\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\", #no for no saving \n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"map@3\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    bf16=False, # TRAIN WITH BF16 IF LOCAL GPU IS NEWER GPU          \n",
    "    fp16=True, # INFER WITH FP16 BECAUSE KAGGLE IS T4 GPU\n",
    ")\n",
    "# CUSTOM MAP@3 METRIC\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def compute_map3(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    \n",
    "    top3 = np.argsort(-probs, axis=1)[:, :3]  # Top 3 predictions\n",
    "    match = (top3 == labels[:, None])\n",
    "\n",
    "    # Compute MAP@3 manually\n",
    "    map3 = 0\n",
    "    for i in range(len(labels)):\n",
    "        if match[i, 0]:\n",
    "            map3 += 1.0\n",
    "        elif match[i, 1]:\n",
    "            map3 += 1.0 / 2\n",
    "        elif match[i, 2]:\n",
    "            map3 += 1.0 / 3\n",
    "    return {\"map@3\": map3 / len(labels)}\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_map3,\n",
    ")\n",
    "\n",
    "# Load test set\n",
    "test = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "# HuggingFace Dataset\n",
    "ds_test = Dataset.from_pandas(test[['text']])\n",
    "ds_test = ds_test.map(tokenize, batched=True)\n",
    "\n",
    "# Run inference\n",
    "pred_output = trainer.predict(ds_test)\n",
    "logits = pred_output.predictions\n",
    "probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# ---- Top-3 predictions ----\n",
    "top3 = np.argsort(-probs, axis=1)[:, :3]   # shape: [num_samples, 3]\n",
    "\n",
    "# Decode numeric class indices to original string labels\n",
    "flat_top3 = top3.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_top3)\n",
    "top_labels = decoded_labels.reshape(top3.shape)\n",
    "\n",
    "# Join 3 labels per row with space\n",
    "joined_preds = [\" \".join(row) for row in top_labels]\n",
    "\n",
    "# Save Top-3 submission\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission_qwen3_14b.csv\", index=False)\n",
    "\n",
    "# ---- Save Top-25 probability table (for ensembling) ----\n",
    "prob_data = []\n",
    "top_k = 25\n",
    "top_indices = np.argsort(-probs, axis=1)\n",
    "\n",
    "for i in range(len(logits)):\n",
    "    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(top_k)}\n",
    "    prob_dict[\"row_id\"] = test.row_id.values[i]\n",
    "    prob_dict[\"top_classes\"] = \" \".join(\n",
    "        le.inverse_transform(top_indices[i, :top_k])\n",
    "    )\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_qwen3_14b_prob.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b575c7",
   "metadata": {
    "papermill": {
     "duration": 0.00307,
     "end_time": "2025-10-16T17:27:10.170295",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.167225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# qwen3 deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b36ff7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:27:10.177702Z",
     "iopub.status.busy": "2025-10-16T17:27:10.177513Z",
     "iopub.status.idle": "2025-10-16T17:27:10.183313Z",
     "shell.execute_reply": "2025-10-16T17:27:10.182645Z"
    },
    "papermill": {
     "duration": 0.011014,
     "end_time": "2025-10-16T17:27:10.184425",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.173411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_deepseek_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_deepseek_inference.py\n",
    "\n",
    "# we do parallel inference, for deepseek and qwen3\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "import threading\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "model_paths = [\n",
    "    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",\n",
    "   \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"]\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"This answer is correct.\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"This is answer is incorrect.\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"{x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\")\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "train.Misconception  = train.Misconception.fillna('NA')\n",
    "train['target']   = train.Category + ':' +train.Misconception\n",
    "train['label']    = le.fit_transform(train['target'])\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input,axis=1)\n",
    "ds_test = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "def run_inference_on_gpu(model_path, gpu_id, test_data, output_name):\n",
    "    \"\"\"Run inference for one model on one GPU\"\"\"\n",
    "    \n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "    print(f\"Loading {output_name} on {device}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, \n",
    "        device_map=device, \n",
    "        dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], \n",
    "                        truncation=True,\n",
    "                        max_length=256)\n",
    "    \n",
    "    ds_test = Dataset.from_pandas(test_data[['text']])\n",
    "    ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        pin_memory=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Inference\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"{output_name}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            all_logits.append(outputs.logits.float().cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_logits, axis=0)\n",
    "    \n",
    "    # Process results\n",
    "    probs = softmax(predictions, axis=1)\n",
    "    top_indices = np.argsort(-probs, axis=1)\n",
    "    \n",
    "    # Decode labels\n",
    "    flat_indices = top_indices.flatten()\n",
    "    decoded_labels = le.inverse_transform(flat_indices)\n",
    "    top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "    \n",
    "    # Save top-3 submission\n",
    "    joined_preds = [\" \".join(row[:3]) for row in top_labels]\n",
    "    sub = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.values,\n",
    "        \"Category:Misconception\": joined_preds\n",
    "    })\n",
    "    sub.to_csv(f\"submission_{output_name}.csv\", index=False)\n",
    "    \n",
    "    # Save probabilities for ensemble\n",
    "    prob_data = []\n",
    "    for i in range(len(predictions)):\n",
    "        prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}\n",
    "        prob_dict['row_id'] = test_data.row_id.values[i]\n",
    "        prob_dict['top_classes'] = \" \".join(top_labels[i, :25])\n",
    "        prob_data.append(prob_dict)\n",
    "    \n",
    "    prob_df = pd.DataFrame(prob_data)\n",
    "    prob_df.to_csv(f\"submission_{output_name}_probabilities.csv\", index=False)\n",
    "    \n",
    "    print(f\" {output_name} completed - saved submission and probabilities\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\" Starting multi-GPU inference...\")\n",
    "start_time = time.time()\n",
    "\n",
    "threads = []\n",
    "gpu_assignments = [\n",
    "    (model_paths[0], 0, \"deepseek\"),\n",
    "    (model_paths[1], 1, \"qwen3\"),\n",
    "]\n",
    "\n",
    "# Start threads\n",
    "for model_path, gpu_id, name in gpu_assignments:\n",
    "    if gpu_id < torch.cuda.device_count():  \n",
    "        thread = threading.Thread(\n",
    "            target=run_inference_on_gpu,\n",
    "            args=(model_path, gpu_id, test, name)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        time.sleep(10)  # Stagger starts to avoid memory issues\n",
    "\n",
    "# Wait for completion\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\" completed in {end_time - start_time:.2f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab55c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:27:10.191417Z",
     "iopub.status.busy": "2025-10-16T17:27:10.191219Z",
     "iopub.status.idle": "2025-10-16T17:38:23.954865Z",
     "shell.execute_reply": "2025-10-16T17:38:23.953881Z"
    },
    "papermill": {
     "duration": 673.768783,
     "end_time": "2025-10-16T17:38:23.956349",
     "exception": false,
     "start_time": "2025-10-16T17:27:10.187566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-16 17:27:21.018775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760635641.256103      61 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760635641.325937      61 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:11<00:00, 17.80s/it]\r\n",
      "Some weights of HunYuanDenseV1ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/hunyuan-7b-instruct-bf16 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 114.30 examples/s]\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3968.12it/s]\r\n",
      "2025-10-16 17:29:16.500027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760635756.522626      84 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760635756.529463      84 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:38<00:00, 19.35s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 205.30 examples/s]\r\n",
      "deepseek: 100%|███████████████████████████████████| 1/1 [00:00<00:00,  2.40it/s]\r\n",
      "✅ Completed - saved submission and probabilities\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      "Example prompt for our LLM:\r\n",
      "\r\n",
      "Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.]\r\n",
      "Answer: \\( \\frac{1}{3} \\)\r\n",
      "Is Correct Answer: Yes\r\n",
      "Student Explanation: 0ne third is equal to tree nineth\r\n",
      "Map: 100%|███████████████████████| 29356/29356 [00:05<00:00, 5578.08 examples/s]\r\n",
      "Map: 100%|█████████████████████████| 7340/7340 [00:01<00:00, 5113.58 examples/s]\r\n",
      "2025-10-16 17:30:28.388441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760635828.413212     107 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760635828.420078     107 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:12<00:00, 36.05s/it]\r\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/qwen-2-5-map and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "torch.float16\r\n",
      "Map: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  7.22 examples/s]\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2995.93it/s]\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      "Example prompt for our LLM:\r\n",
      "\r\n",
      "Question: What fraction of the shape is not shaded? Give your answer in its simplest form. [Image: A triangle split into 9 equal smaller triangles. 6 of them are shaded.]\r\n",
      "Answer: \\( \\frac{1}{3} \\)\r\n",
      "Is Correct Answer: Yes\r\n",
      "Student Explanation: 0ne third is equal to tree nineth\r\n",
      "Map: 100%|███████████████████████| 29356/29356 [00:05<00:00, 5564.47 examples/s]\r\n",
      "Map: 100%|█████████████████████████| 7340/7340 [00:01<00:00, 5127.47 examples/s]\r\n",
      "2025-10-16 17:32:39.457362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760635959.482693     137 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760635959.489622     137 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:23<00:00, 17.89s/it]\r\n",
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/qwen-3/transformers/14b/1 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "torch.float16\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 431.65 examples/s]\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3734.91it/s]\r\n",
      "2025-10-16 17:35:45.277924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760636145.300279     167 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760636145.307972     167 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      " Starting multi-GPU inference...\r\n",
      "Loading deepseek on cuda:0...\r\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]Loading qwen3 on cuda:1...\r\n",
      "\r\n",
      "Loading checkpoint shards:  33%|██████            | 1/3 [00:42<01:25, 42.90s/it]\r\n",
      "Loading checkpoint shards:  67%|████████████      | 2/3 [01:26<00:43, 43.52s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:51<00:00, 37.27s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 427.98 examples/s]\r\n",
      "deepseek: 100%|███████████████████████████████████| 1/1 [00:01<00:00,  1.49s/it]\r\n",
      " deepseek completed - saved submission and probabilities\r\n",
      "\r\n",
      "Loading checkpoint shards:  75%|█████████████▌    | 3/4 [02:08<00:42, 42.88s/it]\u001b[A\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:11<00:00, 32.81s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 726.54 examples/s]\r\n",
      "qwen3: 100%|██████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\r\n",
      " qwen3 completed - saved submission and probabilities\r\n",
      " completed in 142.70 seconds!\r\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "!python /kaggle/working/Hunyaun_inference.py\n",
    "time.sleep(10)\n",
    "!python /kaggle/working/qwen3_4b_inference.py\n",
    "time.sleep(10)\n",
    "!python /kaggle/working/qwen2_8b.py\n",
    "time.sleep(10)\n",
    "!python /kaggle/working/qwen3_14B.py\n",
    "time.sleep(10)\n",
    "!python /kaggle/working/qwen3_deepseek_inference.py\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1338b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:38:23.976113Z",
     "iopub.status.busy": "2025-10-16T17:38:23.975888Z",
     "iopub.status.idle": "2025-10-16T17:38:24.508715Z",
     "shell.execute_reply": "2025-10-16T17:38:24.507895Z"
    },
    "papermill": {
     "duration": 0.544135,
     "end_time": "2025-10-16T17:38:24.509840",
     "exception": false,
     "start_time": "2025-10-16T17:38:23.965705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id                             Category:Misconception\n",
      "0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  True_Neither:NA True_Correct:NA True_Misconcep...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------\n",
    "# Build family map\n",
    "# -------------------------\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "train['is_true'] = train['Category'].str.startswith('True')\n",
    "correct = (train[train.is_true]\n",
    "           .assign(c=lambda df: df.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count'))\n",
    "           .sort_values('c', ascending=False)\n",
    "           .drop_duplicates(['QuestionId'])[['QuestionId','MC_Answer']])\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "fam_map = (test_df.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "                  .assign(is_correct=lambda df: df.is_correct.fillna(0).astype(int))\n",
    "                  .set_index('row_id')['is_correct']\n",
    "                  .map({1: 'True_', 0: 'False_'}).to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# Ensemble\n",
    "# -------------------------\n",
    "def extract_class_probabilities(row, model_suffix='', top_k=25):\n",
    "    \"\"\"Extract class names and probabilities from a row\"\"\"\n",
    "    classes_col = f'top_classes{model_suffix}'\n",
    "    if classes_col in row:\n",
    "        classes = row[classes_col].split(' ')[:top_k]\n",
    "    else:\n",
    "        return {}\n",
    "    class_probs = {}\n",
    "    for i in range(min(top_k, len(classes))):\n",
    "        prob_col = f'prob_{i}{model_suffix}'\n",
    "        if prob_col in row:\n",
    "            class_probs[classes[i]] = row[prob_col]\n",
    "    return class_probs\n",
    "\n",
    "\n",
    "def ensemble_with_disagreement_handling(prob_files, model_weights=None, top_k=3):\n",
    "    n_models = len(prob_files)\n",
    "    prob_dfs = []\n",
    "    final_predictions = []\n",
    "    \n",
    "    for file_path in prob_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        prob_dfs.append(df)\n",
    "    \n",
    "    # Merge on row_id\n",
    "    merged_df = prob_dfs[0]\n",
    "    for i, df in enumerate(prob_dfs[1:], 1):\n",
    "        merged_df = pd.merge(merged_df, df, on='row_id', suffixes=('', f'_model{i+1}'))\n",
    "      \n",
    "    for idx, row in merged_df.iterrows():\n",
    "        pref = fam_map[row['row_id']]  # family for this row\n",
    "        \n",
    "        # Extract probabilities from each model\n",
    "        all_class_probs = []\n",
    "        for i in range(n_models):\n",
    "            suffix = f'_model{i+1}' if i > 0 else ''\n",
    "            class_probs = extract_class_probabilities(row, suffix, top_k=25)\n",
    "            all_class_probs.append(class_probs)\n",
    "        \n",
    "        # Get all unique classes\n",
    "        all_classes = set()\n",
    "        for class_probs in all_class_probs:\n",
    "            all_classes.update(class_probs.keys())\n",
    "        \n",
    "        # Calculate scores\n",
    "        class_votes = defaultdict(int)\n",
    "        class_total_prob = defaultdict(float)\n",
    "        class_max_prob = defaultdict(float)\n",
    "        \n",
    "        for i, class_probs in enumerate(all_class_probs):\n",
    "            weight = model_weights[i]\n",
    "            for class_name, prob in class_probs.items():\n",
    "                class_votes[class_name] += 1\n",
    "                class_total_prob[class_name] += prob * weight\n",
    "                class_max_prob[class_name] = max(class_max_prob[class_name], prob * weight)\n",
    "        \n",
    "        final_scores = {}\n",
    "        for class_name in all_classes:\n",
    "            base_score = class_total_prob[class_name]\n",
    "            agreement_bonus = class_votes[class_name] / n_models\n",
    "            confidence_bonus = class_max_prob[class_name]\n",
    "            final_scores[class_name] = (\n",
    "                base_score * 0.34 +\n",
    "                agreement_bonus * 0.33 +\n",
    "                confidence_bonus * 0.33\n",
    "            )\n",
    "        \n",
    "        # -------------------------\n",
    "        # Family filter\n",
    "        # -------------------------\n",
    "        final_scores = {k: v for k, v in final_scores.items() if k.startswith(pref)}\n",
    "        \n",
    "        # Sort and get top-k\n",
    "        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n",
    "        top_classes = [class_name for class_name, _ in sorted_classes[:top_k]]\n",
    "        \n",
    "        # Backfill if < 3\n",
    "        fillers = [f\"{pref}Neither:NA\"] + ([f\"{pref}Correct:NA\"] if pref == \"True_\" else [])\n",
    "        for f in fillers:\n",
    "            if len(top_classes) >= 3: break\n",
    "            if f not in top_classes:\n",
    "                top_classes.append(f)\n",
    "        while len(top_classes) < 3:\n",
    "            top_classes.append(fillers[0])\n",
    "        \n",
    "        final_predictions.append(' '.join(top_classes))\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Run ensemble\n",
    "# -------------------------\n",
    "weights = [\n",
    "    1,  # Qwen3 4B LoRA - 0.945 (best performer)\n",
    "    1,  # Hunyuan - 0.945 (best performer)\n",
    "    1,  # Qwen2 8B - 0.945 (BF)\n",
    "    1,  # Qwen3 14B - 0.944 \n",
    "    1,  # DeepseekMath 7B - 0.944\n",
    "    1,  # Qwen 8B - 0.943\n",
    "]\n",
    "\n",
    "prob_files = [\n",
    "    '/kaggle/working/submission_qwen3_4B_probabilities.csv',   # 0.945 (Qwen 4B LoRA)\n",
    "    '/kaggle/working/submission_Hunyaun_prob.csv',             # 0.945 (Hunyuan)\n",
    "    '/kaggle/working/submission_qwen2_8b_prob.csv',            # 0.945 (Qwen2 8B)\n",
    "    '/kaggle/working/submission_qwen3_14b_prob.csv',           # 0.944 (Qwen3 14B)\n",
    "    '/kaggle/working/submission_deepseek_probabilities.csv',   # 0.944 (DeepSeek Math 7B)\n",
    "    '/kaggle/working/submission_qwen3_probabilities.csv'       # 0.943 (Qwen 3 8B)\n",
    "]\n",
    "\n",
    "predictions = ensemble_with_disagreement_handling(\n",
    "    prob_files, \n",
    "    model_weights=weights,  \n",
    "    top_k=8\n",
    ")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_df.row_id.values,\n",
    "    'Category:Misconception': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 8039184,
     "sourceId": 12719174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8045877,
     "sourceId": 12729471,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8343008,
     "sourceId": 13166532,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8343060,
     "sourceId": 13166597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8343064,
     "sourceId": 13166603,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8359914,
     "sourceId": 13191698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8370500,
     "sourceId": 13206949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8417764,
     "sourceId": 13282317,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8421142,
     "sourceId": 13287417,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301527,
     "sourceId": 363149,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 692.568905,
   "end_time": "2025-10-16T17:38:24.835996",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T17:26:52.267091",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
